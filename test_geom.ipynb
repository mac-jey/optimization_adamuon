{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f69c06d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9827eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextwrap\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ── Custom optimizer stack (Normalizer / Bridge / Stepper / Gradient / Preconditioner / Memory)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m boundary                       \u001b[38;5;66;03m# x∈[L,U]^d ↔ y∈[0,1]^d 매핑\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBridge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OnePointOptimizer, BridgeConfig   \u001b[38;5;66;03m# 단일-루프 최적화 오케스트레이터\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStepper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StepperConfig, AdamState         \u001b[38;5;66;03m# AdamW×Muon + TR/백트래킹 정책\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Normalize'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.constants import c\n",
    "from scipy.optimize import minimize\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import threading, time, ctypes\n",
    "from pywinauto import Desktop\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "# ── Custom optimizer stack (Normalizer / Bridge / Stepper / Gradient / Preconditioner / Memory)\n",
    "from Normalize import boundary                       # x∈[L,U]^d ↔ y∈[0,1]^d 매핑\n",
    "from Bridge import OnePointOptimizer, BridgeConfig   # 단일-루프 최적화 오케스트레이터\n",
    "from Stepper import StepperConfig, AdamState         # AdamW×Muon + TR/백트래킹 정책\n",
    "from preconditioner import PreconditionerConfig      # 좌표 스케일/회전 안정화\n",
    "from Gradient import Forward1P, Central2P            # 유한차분 1P/2P 그라디언트 소스\n",
    "from Memory import MemoryMetric, MemoryEvent         # 수용/진동 신호 기록\n",
    "import math, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5074b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Lumerical\\v251\\api\\python\\lumapi.py:895: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  message = re.sub('^(Error:)\\s(prompt line)\\s[0-9]+:', '', str(rvals[2])).strip()\n"
     ]
    }
   ],
   "source": [
    "lumapi_path = r\"C:\\Program Files\\Lumerical\\v251\\api\\python\"\n",
    "if lumapi_path not in sys.path:\n",
    "    sys.path.append(lumapi_path)\n",
    "\n",
    "import lumapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b827789",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = 1e-3\n",
    "um = 1e-6\n",
    "nm = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815eb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_thread_started = False\n",
    "\n",
    "def remove_popup():\n",
    "    global _thread_started\n",
    "    if _thread_started: return\n",
    "    _thread_started = True\n",
    "\n",
    "    def _worker():\n",
    "        d = Desktop(backend='uia')\n",
    "        while True:\n",
    "            for w in d.windows():\n",
    "                if w.window_text().strip() == \"Select frequency\":\n",
    "                    ctypes.windll.user32.AllowSetForegroundWindow(-1)\n",
    "                    try:\n",
    "                        w.minimize(); time.sleep(0.1); w.restore(); time.sleep(0.1)\n",
    "                    except: pass\n",
    "                    for b in w.descendants(control_type=\"Button\"):\n",
    "                        if b.window_text() == \"All\":\n",
    "                            try: b.invoke()\n",
    "                            except: b.click_input()\n",
    "                            break\n",
    "                        elif b.window_text() == \"OK\":\n",
    "                            try: b.invoke()\n",
    "                            except: b.click_input()\n",
    "                            break\n",
    "\n",
    "    threading.Thread(target=_worker, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mon(fdtd, mon:str):\n",
    "    # raw_datasets\n",
    "    raw_datasets = fdtd.getresult(mon)   # e.g. [\"Ex\", \"\\n\", \"Ey\", \"\\n\", \"Ez\", ...]\n",
    "\n",
    "    # integrating string\n",
    "    combined = \"\".join(raw_datasets)      # \"Ex\\nEy\\nEz...\"\n",
    "\n",
    "    # organize label\n",
    "    datasets = [ds for ds in combined.split(\"\\n\") if ds]  \n",
    "\n",
    "    # data extract\n",
    "    results = {}\n",
    "    for ds in datasets:\n",
    "        try:\n",
    "            remove_popup()\n",
    "            results[ds] = fdtd.getresult(mon, ds)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # organize dataframe\n",
    "    df_dict = {}\n",
    "    length = None\n",
    "    for key, arr in results.items():\n",
    "        arr1d = np.array(arr).ravel()\n",
    "        if arr1d.ndim == 1:\n",
    "            if length is None:\n",
    "                length = arr1d.shape[0]\n",
    "            if arr1d.shape[0] == length:\n",
    "                df_dict[key] = arr1d\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84da58b",
   "metadata": {},
   "source": [
    "## Plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(df, key=None, comp=None, norm=True, sum_sq=False):\n",
    "    \"\"\"\n",
    "    df     : data frame\n",
    "    key    : keyword of dataframe label\n",
    "    comp   : component of data ; Polarization, Direction\n",
    "    norm   : Normalization data\n",
    "    sum_sq : Sum sequence\n",
    "    \"\"\"\n",
    "\n",
    "    if key not in df.columns:\n",
    "        TypeError(f\"ERROR: '{key}' No column\"); return\n",
    "    entry = df[key].iloc[0]\n",
    "    x = np.array(entry.get('x', []))\n",
    "    y = np.array(entry.get('y', []))\n",
    "    data3d = np.squeeze(np.array(entry[key]))  # (Nx,Ny,3)\n",
    "    if data3d.ndim!=3 or data3d.shape[2]!=3:\n",
    "        TypeError(\"Data shape error:\", data3d.shape); return\n",
    "\n",
    "    if sum_sq:\n",
    "        Z = np.sum(np.abs(data3d)**2, axis=2)\n",
    "        title, cbar = f\"{key} sum_sq\", \"sum_sq\"\n",
    "    elif norm:\n",
    "        Z = np.linalg.norm(data3d, axis=2)\n",
    "        title, cbar = f\"{key} norm\", \"norm\"\n",
    "    else:\n",
    "        c = comp or 0\n",
    "        Z = data3d[:,:,c]\n",
    "        title, cbar = f\"{key}[{c}]\", f\"comp{c}\"\n",
    "\n",
    "    if np.iscomplexobj(Z):\n",
    "        Z = np.abs(Z)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "    plt.figure(figsize=(6,5))\n",
    "    pcm = plt.pcolormesh(X, Y, Z, shading='auto', cmap='magma')\n",
    "    plt.colorbar(pcm, label=cbar)\n",
    "    plt.xlabel('x (m)'); plt.ylabel('y (m)'); plt.title(title)\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df495be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_farfield(ff, lam_idx=0, cmap='magma'):\n",
    "    \"\"\"\n",
    "    Far-field \n",
    "    ff : data of farfield\n",
    "    lam_idx : index of wavelength\n",
    "    cmap : name of colormap\n",
    "    \"\"\"\n",
    "    # 방향 코사인\n",
    "    ux = np.array(ff['ux'])\n",
    "    uy = np.array(ff['uy'])\n",
    "    # intensity 및 편광\n",
    "    E2_all = np.array(ff['E2'])    # shape (Nux, Nuy, 1, Nλ)\n",
    "    Ep_all = np.array(ff['Ep'])\n",
    "    Es_all = np.array(ff['Es'])\n",
    "    \n",
    "    # 불필요 차원 제거 및 파장 인덱스 선택\n",
    "    E2 = np.squeeze(E2_all[:, :, 0, lam_idx])\n",
    "    Ep = np.squeeze(Ep_all[:, :, 0, lam_idx])\n",
    "    Es = np.squeeze(Es_all[:, :, 0, lam_idx])\n",
    "    \n",
    "    # 2D 맵 계산\n",
    "    E2_map = np.real(E2)\n",
    "    Ep_map = np.abs(Ep)\n",
    "    Es_map = np.abs(Es)\n",
    "    \n",
    "    # 그리드 생성\n",
    "    X, Y = np.meshgrid(ux, uy, indexing='ij')\n",
    "    \n",
    "    # 서브플롯\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    titles = ['Total Intensity (E2)', 'P-Polarized |Ep|', 'S-Polarized |Es|']\n",
    "    maps   = [E2_map, Ep_map, Es_map]\n",
    "    labels = ['E2', '|Ep|', '|Es|']\n",
    "    \n",
    "    for ax, Z, title, lbl in zip(axes, maps, titles, labels):\n",
    "        pcm = ax.pcolormesh(X, Y, Z, shading='auto', cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('ux')\n",
    "        ax.set_ylabel('uy')\n",
    "        fig.colorbar(pcm, ax=ax, label=lbl)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93fe70",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d508cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_farfield(mon: dict, src: dict, filter_type: str='gauss', NA: float=0.12, threshold: float=0.95) -> dict:\n",
    "    import numpy as np\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _radial_filter(rho, kind, scale):\n",
    "        if kind == 'flat':    return (rho <= scale).astype(float)\n",
    "        if kind == 'gauss':   return np.exp(-(rho*rho)/(2.0*scale*scale))\n",
    "        if kind == 'sech':    return 1.0/np.cosh(rho/scale)\n",
    "        if kind == 'lorentz': return 1.0/(1.0 + (rho/scale)**2)\n",
    "        raise ValueError(f\"Unknown filter_type: {kind}\")\n",
    "\n",
    "    def _scale_for_cover(rho, NA, cover, kind):\n",
    "        rho   = np.asarray(rho)\n",
    "        NA    = float(np.clip(NA, 0.0, 1.0))\n",
    "        cover = float(np.clip(cover, 0.0, 1.0))\n",
    "        mNA   = (rho <= NA)\n",
    "        mall  = (rho <= 1.0)\n",
    "        def frac(s):\n",
    "            F = _radial_filter(rho, kind, s)\n",
    "            num = float((F * mNA ).sum())\n",
    "            den = float((F * mall).sum())\n",
    "            return 0.0 if den <= 0.0 else (num/den)\n",
    "        lo, hi = (0.0, 1.0) if kind=='flat' else (1e-6, max(5.0*max(1e-3, NA), 1.0))\n",
    "        for _ in range(64):\n",
    "            mid = 0.5*(lo+hi)\n",
    "            if frac(mid) < cover: hi = mid\n",
    "            else:                  lo = mid\n",
    "        return 0.5*(lo+hi)\n",
    "\n",
    "    def _get_complex(ffdict, base):\n",
    "        r, i = base+'_r', base+'_i'\n",
    "        if (r in ffdict) and (i in ffdict):\n",
    "            return np.asarray(ffdict[r]) + 1j*np.asarray(ffdict[i])\n",
    "        elif base in ffdict:\n",
    "            return np.asarray(ffdict[base])\n",
    "        return None\n",
    "\n",
    "    def _slice_best_2d(A, best_idx):\n",
    "        A = np.asarray(A)\n",
    "        sel = A[..., best_idx]\n",
    "        # (최소한의 축 합: 동일 결과, 불필요한 누적 제거)\n",
    "        while sel.ndim > 2:\n",
    "            sel = sel.sum(axis=0)\n",
    "        return sel  # (Nx, Ny)\n",
    "\n",
    "    def _ascend(x, y=None):\n",
    "        x = np.asarray(x).ravel()\n",
    "        if x.size >= 2 and x[0] > x[-1]:\n",
    "            x = x[::-1]\n",
    "            if y is None:\n",
    "                return x, None\n",
    "            if isinstance(y, (list, tuple)):\n",
    "                return x, [np.asarray(v)[::-1] for v in y]\n",
    "            return x, np.asarray(y)[::-1]\n",
    "        return x, y\n",
    "\n",
    "    # NEW: 안전 1D 보간기 -----------------------------------------------\n",
    "    def _interp1d_safe(x, xp, fp):\n",
    "        \"\"\"np.interp 래퍼: 1D float 강제, NaN 제거, 정렬/중복 처리.\"\"\"\n",
    "        x  = np.asarray(x,  dtype=float).ravel()\n",
    "        xp = np.asarray(xp, dtype=float).ravel()\n",
    "        fp = np.asarray(fp, dtype=float).ravel()\n",
    "\n",
    "        m = np.isfinite(xp) & np.isfinite(fp)\n",
    "        xp, fp = xp[m], fp[m]\n",
    "        if xp.size == 0:\n",
    "            return np.full_like(x, np.nan)\n",
    "        if xp.size == 1:\n",
    "            return np.full_like(x, fp[0])\n",
    "\n",
    "        # 오름차순 정렬\n",
    "        idx = np.argsort(xp, kind='mergesort')\n",
    "        xp, fp = xp[idx], fp[idx]\n",
    "\n",
    "        # 중복 xp 평균병합\n",
    "        if xp.size > 1 and np.any(np.diff(xp) == 0):\n",
    "            xu, inv = np.unique(xp, return_inverse=True)\n",
    "            fp_sum = np.zeros_like(xu)\n",
    "            cnt    = np.zeros_like(xu)\n",
    "            np.add.at(fp_sum, inv, fp)\n",
    "            np.add.at(cnt,    inv, 1)\n",
    "            xp, fp = xu, fp_sum / np.maximum(cnt, 1)\n",
    "\n",
    "        return np.interp(x, xp, fp)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # --- 0) farfield grid & rho ---\n",
    "    ff  = mon['farfield'][0]\n",
    "    ux  = np.asarray(ff['ux']).astype(float).ravel()\n",
    "    uy  = np.asarray(ff['uy']).astype(float).ravel()\n",
    "    Ux, Uy = np.meshgrid(ux, uy, indexing='ij')\n",
    "    rho    = np.hypot(Ux, Uy)\n",
    "\n",
    "    # --- 1) intensity cube ---\n",
    "    E2    = np.asarray(ff['E2'], dtype=float)  # (Nx,Ny,Nz,Nf)\n",
    "    I_sum = E2.sum(axis=2)                     # (Nx,Ny,Nf)\n",
    "    Nx, Ny, Nf = I_sum.shape\n",
    "    V = np.ascontiguousarray(I_sum.reshape(Nx*Ny, Nf))   # (M,Nf)\n",
    "    sums_total = V.sum(axis=0).astype(float)\n",
    "    sums_total[sums_total == 0] = np.nan\n",
    "\n",
    "    # --- 2) filters ---\n",
    "    cover = threshold\n",
    "    def _build_filter(rho, kind, NA, cover):\n",
    "        if kind == 'flat':\n",
    "            Fk = (rho <= NA).astype(float); scale = NA\n",
    "        else:\n",
    "            scale = _scale_for_cover(rho, NA, cover, kind)\n",
    "            Fk = _radial_filter(rho, kind, scale).astype(float)\n",
    "        return Fk.ravel(), float(scale)\n",
    "\n",
    "    F_g, _ = _build_filter(rho, 'gauss',   NA, cover)\n",
    "    F_l, _ = _build_filter(rho, 'lorentz', NA, cover)\n",
    "    F_s, _ = _build_filter(rho, 'sech',    NA, cover)\n",
    "    F_f, _ = _build_filter(rho, 'flat',    NA, cover)\n",
    "\n",
    "    # --- 3) CE spectra (단일 matmul) ---\n",
    "    F_mat = np.ascontiguousarray(np.stack([F_g, F_l, F_s, F_f], axis=1))  # (M,4)\n",
    "    CE_all = (V.T @ F_mat) / sums_total[:, None]                           # (Nf,4)\n",
    "    CE_gauss_spec, CE_lorentz_spec, CE_sech_spec, CE_flat_spec = (\n",
    "        CE_all[:,0], CE_all[:,1], CE_all[:,2], CE_all[:,3]\n",
    "    )\n",
    "\n",
    "    # --- 4) wavelength alignment ---\n",
    "    lam_P, PUR = _ascend(src['purcell'][0]['lambda'], src['purcell'][0]['purcell'])\n",
    "    lam_P = np.asarray(lam_P, dtype=float).ravel()\n",
    "    PUR   = np.asarray(PUR,   dtype=float).ravel()\n",
    "\n",
    "    lam_FF, CE_stack = _ascend(ff['lambda'], [CE_gauss_spec, CE_lorentz_spec, CE_sech_spec, CE_flat_spec])\n",
    "    lam_FF = np.asarray(lam_FF, dtype=float).ravel()\n",
    "    CE_gauss_spec, CE_lorentz_spec, CE_sech_spec, CE_flat_spec = [np.asarray(a, dtype=float).ravel() for a in CE_stack]\n",
    "\n",
    "    # 안전 보간기로 교체 (여기가 에러 지점)  -----------------------------\n",
    "    CE_g    = _interp1d_safe(lam_P, lam_FF, CE_gauss_spec)\n",
    "    CE_l    = _interp1d_safe(lam_P, lam_FF, CE_lorentz_spec)\n",
    "    CE_s    = _interp1d_safe(lam_P, lam_FF, CE_sech_spec)\n",
    "    CE_flat = _interp1d_safe(lam_P, lam_FF, CE_flat_spec)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # --- 5) Transmission (raw) ---\n",
    "    lam_T, T_raw = _ascend(mon['T'][0]['lambda'], mon['T'][0]['T'])\n",
    "    lam_T = np.asarray(lam_T, dtype=float).ravel()\n",
    "    T_raw = np.asarray(T_raw, dtype=float).ravel()\n",
    "    T_on_P = _interp1d_safe(lam_P, lam_T, T_raw)\n",
    "    T_norm_sp = np.maximum(T_on_P, 0.0)\n",
    "\n",
    "    # --- 6) best wavelength ---\n",
    "    best_idx = int(np.nanargmax(PUR))\n",
    "    best_lam = float(lam_P[best_idx])\n",
    "    ff_idx   = int(np.nanargmin(np.abs(lam_FF - best_lam)))\n",
    "\n",
    "    # --- 7) Stokes ---\n",
    "    Ep_all = _get_complex(ff, 'Ep')\n",
    "    Es_all = _get_complex(ff, 'Es')\n",
    "    S_0 = S_1 = S_2 = S_3 = np.nan\n",
    "    DoLP = DoCP = np.nan\n",
    "    if (Ep_all is not None) and (Es_all is not None):\n",
    "        Epb = _slice_best_2d(Ep_all, ff_idx)\n",
    "        Esb = _slice_best_2d(Es_all, ff_idx)\n",
    "        Esb_conj = np.conj(Esb)\n",
    "        abs_Epb2 = np.abs(Epb)**2\n",
    "        abs_Esb2 = np.abs(Esb)**2\n",
    "        cross    = Epb * Esb_conj\n",
    "        mNA = (rho <= NA)\n",
    "        S_0 = float(((abs_Epb2 + abs_Esb2) * mNA).sum())\n",
    "        S_1 = float(((abs_Epb2 - abs_Esb2) * mNA).sum())\n",
    "        S_2 = float(((2.0*np.real(cross)) * mNA).sum())\n",
    "        S_3 = float(((-2.0*np.imag(cross)) * mNA).sum())\n",
    "        eps = 1e-300\n",
    "        inv_S0 = 1.0 / (S_0 + eps)\n",
    "        DoLP = np.sqrt(S_1**2 + S_2**2) * inv_S0\n",
    "        DoCP = S_3 * inv_S0\n",
    "\n",
    "    # --- 8) return ---\n",
    "    return {\n",
    "        'best_wl_idx': best_idx,\n",
    "        'lambda':      best_lam,\n",
    "        'CE_g':        float(CE_g[best_idx]),\n",
    "        'CE_l':        float(CE_l[best_idx]),\n",
    "        'CE_s':        float(CE_s[best_idx]),\n",
    "        'CE_flat':     float(CE_flat[best_idx]),\n",
    "        'T_norm':      float(T_norm_sp[best_idx]),\n",
    "        'purcell':     float(PUR[best_idx]),\n",
    "        'S_0':         S_0,\n",
    "        'S_1':         S_1,\n",
    "        'S_2':         S_2,\n",
    "        'S_3':         S_3,\n",
    "        'DoLP':        DoLP,\n",
    "        'DoCP':        DoCP,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Self-contained helpers (NumPy + lumapi; assumes your get_mon/analyze_farfield exist)\n",
    "import numpy as np\n",
    "import lumapi\n",
    "\n",
    "um = 1e-6  # micrometer in meters\n",
    "\n",
    "\n",
    "def _S_mode(t: float, mode: str = \"arch\", gamma: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Monotone shaping map S:[0,1]→[0,1] used for radial spiral profiles.\n",
    "    Inputs:\n",
    "      t     : normalized radius in [0,1]\n",
    "      mode  : {'none','arch','log','clothoid'} profile family\n",
    "      gamma : exponent shaping (>0), applied as S^gamma\n",
    "    Output:\n",
    "      float in [0,1]\n",
    "    \"\"\"\n",
    "    t = max(0.0, min(1.0, float(t)))\n",
    "    if mode == \"none\":\n",
    "        S0 = 1.0\n",
    "    elif mode == \"arch\":\n",
    "        S0 = t\n",
    "    elif mode == \"log\":\n",
    "        S0 = np.log(1.0 + (np.e - 1.0) * t)\n",
    "    elif mode == \"clothoid\":\n",
    "        a, p = 1.0, 1.0\n",
    "        q = p + 1.0\n",
    "        S0 = t if abs(q) < 1e-12 else ((1.0 + a * t) ** q - 1.0) / ((1.0 + a) ** q - 1.0)\n",
    "    else:\n",
    "        S0 = t\n",
    "    g = max(float(gamma), 1e-9)\n",
    "    return float(np.clip(S0, 0.0, 1.0)) ** g\n",
    "\n",
    "\n",
    "def make_spiral_offsets(\n",
    "    ring_num: int,\n",
    "    *,\n",
    "    base_even: float = 0.0, base_odd: float = 0.0,   # constant (× step)\n",
    "    amp_even: float = 0.0,  amp_odd: float = 0.0,    # amplitude (× step)\n",
    "    mode: str = \"arch\", gamma: float = 1.0\n",
    ") -> tuple[list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Create per-ring angular offsets (× step) for even/odd rings following a simple spiral profile.\n",
    "    Returns:\n",
    "      (e_eta_seq, o_eta_seq): lists of length=ring_num. Use with `geom(..., e_eta_seq=..., o_eta_seq=...)`.\n",
    "    \"\"\"\n",
    "    if ring_num <= 0:\n",
    "        return [], []\n",
    "    e_seq, o_seq = [], []\n",
    "    for i in range(ring_num):\n",
    "        t = 0.0 if ring_num == 1 else i / (ring_num - 1)  # normalized radius\n",
    "        S = _S_mode(t, mode=mode, gamma=gamma)\n",
    "        e_seq.append(base_even + amp_even * S)\n",
    "        o_seq.append(base_odd  + amp_odd  * S)\n",
    "    return e_seq, o_seq\n",
    "\n",
    "\n",
    "def geom(\n",
    "    a_phi_m: float, hole_radius_m: float, N0: int, delta_N: int, ring_num: int,\n",
    "    wl_i_m: float, wl_f_m: float,\n",
    "    *,\n",
    "    # per-ring offsets (×step) from make_spiral_offsets()\n",
    "    e_eta_seq: list[float] | None = None,\n",
    "    o_eta_seq: list[float] | None = None,\n",
    "    # parity & radius compensation\n",
    "    parity_mat: bool = True,       # True: odd rings remove exactly one hole (2n→2n-1)\n",
    "    rho: float = 0.5,              # r' = r * (N_after/N_before)**rho\n",
    "    # IO\n",
    "    NA: float = 0.12,\n",
    "    f_name: str = \"temp\",\n",
    "    slab_material: str = \"InP - Palik\",\n",
    "    dpml: float = 0.5 * um,\n",
    "    slab_thickness: float = 0.3 * um,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build and run a hole-type circular Bragg/PhC ring with per-ring spiral offsets (Lumerical FDTD).\n",
    "    Inputs:\n",
    "      - geometry and wavelength band in meters; ring parity and radius compensation controls.\n",
    "    Output:\n",
    "      - Whatever your `analyze_farfield(ff, src, NA=...)` returns (dict of metrics expected).\n",
    "    Notes:\n",
    "      - Requires helper functions `get_mon(session,name)` and `analyze_farfield(ff, src, NA=...)`.\n",
    "      - Updated to current APIs: uses adddftmonitor (replacing deprecated addprofile), sets background index on FDTD.\n",
    "    \"\"\"\n",
    "    # ---------- derived geometry ----------\n",
    "    R0      = (N0 * a_phi_m) / (2 * np.pi)\n",
    "    delta_r = (delta_N * a_phi_m) / (2 * np.pi)\n",
    "    r_max   = R0 + delta_r * (ring_num - 1)\n",
    "    span_xy = (r_max + dpml) * 2 + dpml\n",
    "\n",
    "    # ---------- sanitize offset sequences ----------\n",
    "    e_eta_seq = (e_eta_seq or [0.0] * ring_num)[:ring_num]\n",
    "    o_eta_seq = (o_eta_seq or [0.0] * ring_num)[:ring_num]\n",
    "\n",
    "    # ---------- Lumerical session ----------\n",
    "    # Use supported constructor + serverArgs (offscreen solve).  [Docs: class FDTD + serverArgs]\n",
    "    fdtd = lumapi.FDTD(hide=False)\n",
    "\n",
    "    try:\n",
    "        # Region\n",
    "        fdtd.newproject()\n",
    "        fdtd.addfdtd()                                                # add FDTD region (3D default)  [doc]\n",
    "        fdtd.setnamed(\"FDTD\", \"x span\", span_xy)\n",
    "        fdtd.setnamed(\"FDTD\", \"y span\", span_xy)\n",
    "        fdtd.setnamed(\"FDTD\", \"z span\", 4e-6)\n",
    "        fdtd.setnamed(\"FDTD\", \"index\", 1.0)                # not \"index\" on solver  [doc]\n",
    "\n",
    "        # Slab\n",
    "        fdtd.addcircle(); fdtd.set(\"name\", \"slab\")                    # circle primitive (extruded by z span)  [doc]\n",
    "        fdtd.set(\"x\", 0); fdtd.set(\"y\", 0)\n",
    "        fdtd.set(\"radius\", r_max + dpml)\n",
    "        fdtd.set(\"z span\", slab_thickness)\n",
    "        fdtd.set(\"material\", slab_material)\n",
    "\n",
    "        # Holes as a scripted group (fast object creation)\n",
    "        script = ['addgroup; set(\"name\",\"holes\");']\n",
    "        for i in range(ring_num):\n",
    "            ring_idx = i + 1\n",
    "            Rm = R0 + i * delta_r\n",
    "            Nb = max(int(N0 + i * delta_N), 1)               # baseline count (even: 2n)\n",
    "            is_odd_ring = (ring_idx % 2 == 1)\n",
    "            Na = (Nb - 1) if (parity_mat and is_odd_ring and Nb > 1) else Nb\n",
    "\n",
    "            step    = 2 * np.pi / Na\n",
    "            pitch_i = (2 * np.pi * Rm) / Na\n",
    "            theta0  = (o_eta_seq[i] if is_odd_ring else e_eta_seq[i]) * step\n",
    "\n",
    "            # radius compensation and simple collision guard\n",
    "            r_use = hole_radius_m * (Na / Nb) ** float(rho) if Na != Nb else hole_radius_m\n",
    "            if 2 * r_use > 0.9 * pitch_i:\n",
    "                r_use = 0.45 * pitch_i\n",
    "\n",
    "            for j in range(Na):\n",
    "                th = theta0 + j * step\n",
    "                x = Rm * np.cos(th); y = Rm * np.sin(th)\n",
    "                script.append(\n",
    "                    \"addcircle;\"\n",
    "                    f'set(\"x\",{x}); set(\"y\",{y}); set(\"z span\",{slab_thickness}); '\n",
    "                    f'set(\"radius\",{r_use}); '\n",
    "                    'set(\"material\",\"<Object defined dielectric>\"); set(\"index\",1.0); '\n",
    "                    'addtogroup(\"holes\");'\n",
    "                )\n",
    "        fdtd.eval(\"\\n\".join(script))\n",
    "\n",
    "        # Source (dipole)\n",
    "        fdtd.adddipole(); fdtd.set(\"name\", \"src\")                     # dipole source  [doc]\n",
    "        fdtd.set(\"theta\", 90); fdtd.set(\"phi\", 90)\n",
    "        fdtd.set(\"x\", 0); fdtd.set(\"y\", 0); fdtd.set(\"z\", 0)\n",
    "        fdtd.set(\"wavelength start\", wl_i_m)\n",
    "        fdtd.set(\"wavelength stop\",  wl_f_m)\n",
    "        fdtd.set(\"amplitude\", 1)\n",
    "\n",
    "        # DFT field monitor (replaces deprecated addprofile)\n",
    "        fdtd.adddftmonitor(); fdtd.set(\"name\", \"far\")                 # DFT monitor  [doc]\n",
    "        fdtd.set(\"monitor type\", 7)                                   # 2D z-normal  [doc]\n",
    "        fdtd.set(\"override global monitor settings\", True)\n",
    "        fdtd.set(\"x\", 0); fdtd.set(\"y\", 0); fdtd.set(\"z\", 1e-6)\n",
    "        fdtd.set(\"x span\", span_xy); fdtd.set(\"y span\", span_xy)\n",
    "        fdtd.set(\"frequency points\", 100)\n",
    "\n",
    "        # BC + far-field settings\n",
    "        fdtd.setnamed(\"FDTD\", \"z min bc\", \"Symmetric\")\n",
    "        fdtd.eval('farfieldsettings(\"override near field mesh\",1);')   # global FF settings  [doc]\n",
    "        fdtd.eval('farfieldsettings(\"near field samples per wavelength\",100);')\n",
    "\n",
    "        # Run\n",
    "        fdtd.save(f_name)\n",
    "        fdtd.run()\n",
    "\n",
    "        # Extract\n",
    "        ff  = get_mon(fdtd, \"far\")\n",
    "        src = get_mon(fdtd, \"src\")\n",
    "    finally:\n",
    "        try:\n",
    "            fdtd.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Post\n",
    "    res = analyze_farfield(ff, src, NA=NA)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746ef84",
   "metadata": {},
   "source": [
    "res, src = geom(\n",
    "    a_phi=0.136*um, hole_radius=0.0466*um,\n",
    "    delta_N=8, N0=16, ring_num=12,\n",
    "    wavelength_i=1.28e-6, wavelength_f=1.32e-6,\n",
    "    eta=0.5, rev_tar=\"even\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b92577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Photonic-structure optimizer (CPU-only, single-process)\n",
    "- External interface in nm\n",
    "- Physics called in meters (nm→m inside objective)\n",
    "- Custom optimizer stack: Normalizer + Bridge + Stepper + Gradient + (optional) Preconditioner\n",
    "- Always logs 4 CSV files (params, analysis, FD-gradient samples, zoom)\n",
    "- Prints one human-readable line per evaluation (flush=True) when disp=True\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Imports\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "import os, csv, time, hashlib, traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from threading import RLock\n",
    "\n",
    "# Custom optimizer stack (must exist in working directory)\n",
    "from Normalize import boundary                               # x∈[L,U]^d ↔ y∈[0,1]^d\n",
    "from Bridge import OnePointOptimizer, BridgeConfig           # loop orchestrator\n",
    "from Stepper import StepperConfig, AdamState                 # AdamW×Muon + trust-region policy\n",
    "from preconditioner import PreconditionerConfig              # SPD scaling/rotation (optional)\n",
    "from Gradient import Forward1P, Central2P                    # 1P bootstrap → 2P central FD\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Thread-safe (single process) cache & file I/O\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "_OBJ_CACHE   = {}          # key -> (obj, res)\n",
    "_CACHE_ROUND = 14\n",
    "_CACHE_MAX   = 4096\n",
    "_CACHE_LOCK  = RLock()\n",
    "_FILE_LOCK   = RLock()\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear all objective cache entries (global).\"\"\"\n",
    "    with _CACHE_LOCK:\n",
    "        _OBJ_CACHE.clear()\n",
    "\n",
    "def _digest_seq(seq) -> str:\n",
    "    \"\"\"Stable short id for lists/tuples/None used in cache & logs.\"\"\"\n",
    "    if seq is None:\n",
    "        return \"none\"\n",
    "    arr = np.asarray(seq, float).ravel()\n",
    "    return hashlib.sha1(arr.tobytes()).hexdigest()[:10]\n",
    "\n",
    "def _obj_key(x, N0, delta_N, ring_num, wl_i, wl_f, f_name,\n",
    "             *, e_eta_seq=None, o_eta_seq=None, parity_mat=True, rho=0.5, NA=0.12):\n",
    "    a_phi, hole_radius = map(float, x[:2])\n",
    "    return (\n",
    "        round(a_phi, _CACHE_ROUND), round(hole_radius, _CACHE_ROUND),\n",
    "        int(N0), int(delta_N), int(ring_num),\n",
    "        round(float(wl_i), _CACHE_ROUND), round(float(wl_f), _CACHE_ROUND),\n",
    "        str(f_name),\n",
    "        _digest_seq(e_eta_seq), _digest_seq(o_eta_seq),\n",
    "        bool(parity_mat), round(float(rho), _CACHE_ROUND),\n",
    "        round(float(NA), _CACHE_ROUND),\n",
    "    )\n",
    "\n",
    "def _resolve_offsets(ring_num: int,\n",
    "                     e_eta_seq: list[float] | None,\n",
    "                     o_eta_seq: list[float] | None,\n",
    "                     offsets=None):\n",
    "    \"\"\"\n",
    "    offsets:\n",
    "      - None: 그대로 사용 (시퀀스가 None이면 0으로 채움)\n",
    "      - dict: {base_even, base_odd, amp_even, amp_odd, mode, gamma} → make_spiral_offsets 로 생성\n",
    "      - callable: fn(ring_num) -> (e_seq, o_seq)\n",
    "    NOTE: make_spiral_offsets(...)는 외부 제공 가정.\n",
    "    \"\"\"\n",
    "    if callable(offsets):\n",
    "        e_eta_seq, o_eta_seq = offsets(ring_num)\n",
    "    elif isinstance(offsets, dict):\n",
    "        # make_spiral_offsets는 외부에서 제공\n",
    "        e_eta_seq, o_eta_seq = make_spiral_offsets(\n",
    "            ring_num,\n",
    "            base_even=offsets.get(\"base_even\", 0.0),\n",
    "            base_odd =offsets.get(\"base_odd\",  0.0),\n",
    "            amp_even =offsets.get(\"amp_even\",  0.0),\n",
    "            amp_odd  =offsets.get(\"amp_odd\",   0.0),\n",
    "            mode     =offsets.get(\"mode\",     \"arch\"),\n",
    "            gamma    =offsets.get(\"gamma\",     1.0),\n",
    "        )\n",
    "    # 보정: None이면 0 시퀀스\n",
    "    if e_eta_seq is None: e_eta_seq = [0.0]*ring_num\n",
    "    if o_eta_seq is None: o_eta_seq = [0.0]*ring_num\n",
    "    # 길이 클리핑\n",
    "    e_eta_seq = list(e_eta_seq)[:ring_num]\n",
    "    o_eta_seq = list(o_eta_seq)[:ring_num]\n",
    "    return e_eta_seq, o_eta_seq\n",
    "\n",
    "def _cache_get(key):\n",
    "    with _CACHE_LOCK:\n",
    "        return _OBJ_CACHE.get(key, None)\n",
    "\n",
    "def _cache_set(key, obj, res):\n",
    "    with _CACHE_LOCK:\n",
    "        _OBJ_CACHE[key] = (obj, res)\n",
    "        # FIFO-ish eviction\n",
    "        while len(_OBJ_CACHE) > _CACHE_MAX:\n",
    "            k = next(iter(_OBJ_CACHE))\n",
    "            _OBJ_CACHE.pop(k, None)\n",
    "\n",
    "def _cache_drop_by_fname(fname: str):\n",
    "    \"\"\"Drop only entries of the given run-tag (f_name).\"\"\"\n",
    "    fname = str(fname)\n",
    "    with _CACHE_LOCK:\n",
    "        # f_name is at index 7 in the key tuple\n",
    "        keys = [k for k in list(_OBJ_CACHE.keys()) if len(k) >= 8 and k[7] == fname]\n",
    "        for k in keys:\n",
    "            _OBJ_CACHE.pop(k, None)\n",
    "\n",
    "def _append_csv(path, row, header=None):\n",
    "    \"\"\"\n",
    "    Append 1 row (thread-safe). Create header if file empty.\n",
    "    NOTE: open(..., newline='') per Python csv docs to avoid blank lines on Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with _FILE_LOCK:\n",
    "            need_header = (not os.path.exists(path)) or os.path.getsize(path) == 0\n",
    "            with open(path, 'a', newline='') as f:\n",
    "                w = csv.writer(f)\n",
    "                if need_header and header:\n",
    "                    w.writerow(header)\n",
    "                w.writerow(row)\n",
    "    except Exception as e:\n",
    "        # Never stop optimization due to logging failure\n",
    "        print(f\"[log-warning] csv append failed for {path}: {e}\", flush=True)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Heavy objective (nm input → convert to m internally)\n",
    "#   geom(...) must return 'res' dict with keys used below\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def objective_geom(\n",
    "    x,\n",
    "    N0: int,\n",
    "    delta_N: int,\n",
    "    ring_num: int,\n",
    "    wavelength_i: float,  # nm\n",
    "    wavelength_f: float,  # nm\n",
    "    f_name: str,\n",
    "    *,\n",
    "    e_eta_seq: list[float] | None = None,\n",
    "    o_eta_seq: list[float] | None = None,\n",
    "    offsets=None,               # dict/callable 지원\n",
    "    parity_mat: bool = True,\n",
    "    NA: float = 0.12,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate physical objective for external x=[a_phi[nm], r[nm], rho]. Caches (obj,res).\n",
    "\n",
    "    Inputs\n",
    "    -------\n",
    "    x : array-like length-3 (nm, nm, unitless)\n",
    "    Returns\n",
    "    -------\n",
    "    obj_val : float\n",
    "      Minimization target. Also stores (obj,res) in the global cache for reuse.\n",
    "    \"\"\"\n",
    "    # 오프셋 확정\n",
    "    e_eta_seq, o_eta_seq = _resolve_offsets(ring_num, e_eta_seq, o_eta_seq, offsets)\n",
    "\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    if x.size != 3:\n",
    "        raise ValueError(\"x must be length-3: [a_phi, hole_radius, rho].\")\n",
    "    a_phi, hole_radius, rho_use = x\n",
    "\n",
    "    key = _obj_key(\n",
    "        x, N0, delta_N, ring_num, wavelength_i, wavelength_f, f_name,\n",
    "        e_eta_seq=e_eta_seq, o_eta_seq=o_eta_seq,\n",
    "        parity_mat=parity_mat, rho=rho_use, NA=NA\n",
    "    )\n",
    "    cached = _cache_get(key)\n",
    "    if cached is not None:\n",
    "        return cached[0]\n",
    "\n",
    "    nm = 1e-9\n",
    "    # geom은 외부 제공. res(dict) 반환 가정.\n",
    "    res = geom(\n",
    "        a_phi*nm, hole_radius*nm,\n",
    "        N0, delta_N, ring_num,\n",
    "        float(wavelength_i)*nm, float(wavelength_f)*nm,\n",
    "        e_eta_seq=e_eta_seq, o_eta_seq=o_eta_seq,\n",
    "        parity_mat=parity_mat, rho=float(rho_use),\n",
    "        NA=float(NA), f_name=str(f_name)\n",
    "    )\n",
    "\n",
    "    CE_f   = float(res['CE_flat'])\n",
    "    T_norm = float(res['T_norm'])\n",
    "    pur    = float(res['purcell'])\n",
    "\n",
    "    # Example composite objective (minimize). Replace if needed.\n",
    "    obj_val = - np.exp(np.exp(CE_f**(1/np.pi))) * np.log(1 + T_norm) * np.log(pur)\n",
    "    if not np.isfinite(obj_val):\n",
    "        raise FloatingPointError(\"objective became non-finite\")\n",
    "\n",
    "    _cache_set(key, obj_val, res)\n",
    "    return obj_val\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Optimization wrapper (external nm; internal y∈[0,1]^3 via Normalizer)\n",
    "# Replaces SciPy 'minimize' with custom Bridge/Stepper/Gradient stack\n",
    "# Writes the SAME 4 CSV files (append mode; schema unchanged):\n",
    "#   - opt_history_{method}_params.csv\n",
    "#   - out_history_{method}_analysis.csv\n",
    "#   - opt_history_{method}_obj.csv\n",
    "#   - zoom_history_{method}.csv\n",
    "# Returns: OptimizeResult-compatible dict\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "def optimize_structure(\n",
    "    x0,                   # [a_phi, hole_radius, rho]\n",
    "    N0: int,\n",
    "    delta_N: int,\n",
    "    ring_num: int,\n",
    "    wavelength_i: float,  # nm\n",
    "    wavelength_f: float,  # nm\n",
    "    f_name,               # run-tag printed & cached with results\n",
    "    method='SLSQP',       # kept for preset mapping only (no SciPy path)\n",
    "    disp=False,           # 기본 OFF\n",
    "    *,\n",
    "    e_eta_seq: list[float] | None = None,\n",
    "    o_eta_seq: list[float] | None = None,\n",
    "    offsets=None,               # dict/callable 지원\n",
    "    parity_mat: bool = True,\n",
    "    rho_bounds: tuple[float, float] = (0.0, 1.0),   # rho 경계(결정변수)\n",
    "    NA: float = 0.12,\n",
    "    scale: float | str = \"auto\"  # [m per optimizer-unit] or \"auto\" (logging 용도 유지)\n",
    "):\n",
    "    \"\"\"\n",
    "    Replace SciPy-based inner loop with:\n",
    "      - Normalizer: x[nm]↔y∈[0,1]^3 (경계 집행)\n",
    "      - Penalty: hinge^2 for a_phi - k*r >= 0\n",
    "      - Bridge: 1P bootstrap → 2P 중앙차분\n",
    "      - Stepper: AdamW×Muon + trust region + improve-only 수용\n",
    "    I/O/CSV 스키마와 OptimizeResult 호환성은 기존과 동일하게 유지합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # ── constants & bounds (external nm) ────────────────────────\n",
    "    nm = 1e-9\n",
    "    nm_bounds = [(1.2e2, 3.0e2),    # a_phi in [120, 300] nm\n",
    "                 (4.0e1, 1.5e2)]    # hole_radius in [40, 150] nm\n",
    "    k = 2.5                         # inequality: a_phi - k*r >= 0\n",
    "\n",
    "    # ── baseline internal scale (for logging only; same as legacy code) ─\n",
    "    def _geom_mean(vals):\n",
    "        vals = np.asarray(vals, float)\n",
    "        return float(np.exp(np.mean(np.log(np.clip(vals, 1e-300, None)))))\n",
    "    typ_nm = np.array([np.sqrt(nm_bounds[0][0]*nm_bounds[0][1]),\n",
    "                       np.sqrt(nm_bounds[1][0]*nm_bounds[1][1])], float)\n",
    "    auto_scale_base = _geom_mean(typ_nm) * nm\n",
    "    scale_base = float(auto_scale_base) if (isinstance(scale, str) and str(scale).lower()==\"auto\") else float(scale)\n",
    "\n",
    "    # ── CSV paths & headers (unchanged) ─────────────────────────\n",
    "    params_csv = f'opt_history_{method}_params.csv'\n",
    "    out_csv    = f'out_history_{method}_analysis.csv'\n",
    "    obj_csv    = f'opt_history_{method}_obj.csv'\n",
    "    zoom_csv   = f'zoom_history_{method}.csv'\n",
    "\n",
    "    params_header = ['f_name','a_phi','hole_radius','N0','delta_N','ring_num',\n",
    "                     'NA','parity_mat','rho','e_hash','o_hash',\n",
    "                     'unit_length','scale_m_per_unit','obj']\n",
    "    out_header = [\n",
    "        'f_name','a_phi','hole_radius','N0','delta_N','ring_num',\n",
    "        'lambda','CE_g','CE_l','CE_s','CE_flat','T_norm','purcell',\n",
    "        'S_0','S_1','S_2','S_3','DoLP','DoCP','best_wl_idx',\n",
    "        'NA','parity_mat','rho','e_hash','o_hash',\n",
    "        'unit_length','scale_m_per_unit','obj'\n",
    "    ]\n",
    "    obj_header = [\n",
    "        'f_name','phase','dim','h','a_phi','hole_radius',\n",
    "        'f_xm','f_x','f_xp','g_fw','g_bw','g_central','g_chosen','scheme',\n",
    "        'feasible_m','feasible_0','feasible_p','unit_length','scale_m_per_unit'\n",
    "    ]\n",
    "    zoom_header = ['f_name','iter','y','s_logL_star','z_logL','L','a_t','a_bar','mode','eff_scale_m_per_unit']\n",
    "\n",
    "    # ── offsets resolved once per run ───────────────────────────\n",
    "    e_eta_seq, o_eta_seq = _resolve_offsets(ring_num, e_eta_seq, o_eta_seq, offsets)\n",
    "    e_hash = _digest_seq(e_eta_seq); o_hash = _digest_seq(o_eta_seq)\n",
    "\n",
    "    # ── external variable checks ────────────────────────────────\n",
    "    x0 = np.asarray(x0, float).ravel()\n",
    "    if x0.size != 3:\n",
    "        raise ValueError(\"x0 must be length-3: [a_phi, hole_radius, rho].\")\n",
    "    bounds_ext = [nm_bounds[0], nm_bounds[1], tuple(map(float, rho_bounds))]\n",
    "    bounds_lo = np.array([b[0] for b in bounds_ext], float)\n",
    "    bounds_hi = np.array([b[1] for b in bounds_ext], float)\n",
    "\n",
    "    def _proj_bounds_ext(x_ext):\n",
    "        x = np.asarray(x_ext, float)\n",
    "        return np.minimum(np.maximum(x, bounds_lo), bounds_hi)\n",
    "\n",
    "    def _feasible_ineq(x_ext):\n",
    "        x = np.asarray(x_ext, float)\n",
    "        return (x[0] - k*x[1]) >= 0.0\n",
    "\n",
    "    # ── objective cache hookup (same as legacy) ─────────────────\n",
    "    def _key_for(x_ext):\n",
    "        rho_use = float(x_ext[2])\n",
    "        return _obj_key(\n",
    "            x_ext, N0, delta_N, ring_num, wavelength_i, wavelength_f, f_name,\n",
    "            e_eta_seq=e_eta_seq, o_eta_seq=o_eta_seq,\n",
    "            parity_mat=parity_mat, rho=rho_use, NA=NA\n",
    "        )\n",
    "\n",
    "    _last_eval = {'x_ext': None, 'f': None, 'res': None}\n",
    "\n",
    "    def _ensure_cached(x_ext):\n",
    "        \"\"\"Evaluate objective_geom(x_ext, ...) with cache, return (f,res).\"\"\"\n",
    "        x_ext = _proj_bounds_ext(x_ext)\n",
    "        if _last_eval['x_ext'] is not None and np.allclose(x_ext, _last_eval['x_ext'], rtol=0, atol=0):\n",
    "            return _last_eval['f'], _last_eval['res']\n",
    "        entry = _cache_get(_key_for(x_ext))\n",
    "        if entry is None:\n",
    "            _ = objective_geom(\n",
    "                x_ext, N0, delta_N, ring_num, wavelength_i, wavelength_f, f_name,\n",
    "                e_eta_seq=e_eta_seq, o_eta_seq=o_eta_seq,\n",
    "                offsets=offsets, parity_mat=parity_mat, NA=NA\n",
    "            )\n",
    "            entry = _cache_get(_key_for(x_ext))\n",
    "        f, res = entry\n",
    "        _last_eval.update({'x_ext': x_ext.copy(), 'f': f, 'res': res})\n",
    "        return f, res\n",
    "\n",
    "    # ── CSV helpers (unchanged) ─────────────────────────────────\n",
    "    def _log_params_and_analysis(x_ext, obj_used, res, eff_scale):\n",
    "        unit = \"nm\"; rho_log = float(x_ext[2])\n",
    "        _append_csv(params_csv, [\n",
    "            str(f_name),\n",
    "            f\"{float(x_ext[0]):.6e}\", f\"{float(x_ext[1]):.6e}\",\n",
    "            int(N0), int(delta_N), int(ring_num),\n",
    "            f\"{float(NA):.6e}\", str(bool(parity_mat)), f\"{float(rho_log):.6e}\",\n",
    "            e_hash, o_hash, unit, f\"{float(eff_scale):.6e}\", f\"{float(obj_used):.6e}\"\n",
    "        ], header=params_header)\n",
    "\n",
    "        _append_csv(out_csv, [\n",
    "            str(f_name),\n",
    "            f\"{float(x_ext[0]):.6e}\", f\"{float(x_ext[1]):.6e}\",\n",
    "            int(N0), int(delta_N), int(ring_num),\n",
    "            f\"{float(res['lambda']):.6e}\",\n",
    "            f\"{float(res['CE_g'] ):.6e}\", f\"{float(res['CE_l'] ):.6e}\",\n",
    "            f\"{float(res['CE_s'] ):.6e}\", f\"{float(res['CE_flat']):.6e}\",\n",
    "            f\"{float(res['T_norm']):.6e}\", f\"{float(res['purcell']):.6e}\",\n",
    "            f\"{float(res['S_0']  ):.6e}\", f\"{float(res['S_1']  ):.6e}\",\n",
    "            f\"{float(res['S_2']  ):.6e}\", f\"{float(res['S_3']  ):.6e}\",\n",
    "            f\"{float(res['DoLP'] ):.6e}\", f\"{float(res['DoCP'] ):.6e}\",\n",
    "            int(res['best_wl_idx']),\n",
    "            f\"{float(NA):.6e}\", str(bool(parity_mat)), f\"{float(rho_log):.6e}\",\n",
    "            e_hash, o_hash, unit, f\"{float(eff_scale):.6e}\", f\"{float(obj_used):.6e}\"\n",
    "        ], header=out_header)\n",
    "\n",
    "    def _log_obj_sample(phase, i, h, x_ext, fm, f0, fp, g_fw, g_bw, g_c, g_chosen, scheme, feas_m, feas_0, feas_p, eff_scale):\n",
    "        unit = \"nm\"\n",
    "        row = [\n",
    "            str(f_name), str(phase), (\"\" if i is None else int(i)),\n",
    "            (\"\" if h is None else f\"{float(h):.6e}\"),\n",
    "            f\"{float(x_ext[0]):.6e}\", f\"{float(x_ext[1]):.6e}\",\n",
    "            (\"\" if fm is None else f\"{float(fm):.6e}\"),\n",
    "            (\"\" if f0 is None else f\"{float(f0):.6e}\"),\n",
    "            (\"\" if fp is None else f\"{float(fp):.6e}\"),\n",
    "            (\"\" if g_fw is None else f\"{float(g_fw):.6e}\"),\n",
    "            (\"\" if g_bw is None else f\"{float(g_bw):.6e}\"),\n",
    "            (\"\" if g_c  is None else f\"{float(g_c ):.6e}\"),\n",
    "            (\"\" if g_chosen is None else f\"{float(g_chosen):.6e}\"),\n",
    "            (\"\" if scheme is None else str(scheme)),\n",
    "            str(bool(feas_m)), str(bool(feas_0)), str(bool(feas_p)),\n",
    "            unit, f\"{float(eff_scale):.6e}\"\n",
    "        ]\n",
    "        _append_csv(obj_csv, row, header=obj_header)\n",
    "\n",
    "    def _log_zoom(iter_idx, y_vec, mode, eff_scale, a_t=0.0, a_bar=0.0):\n",
    "        # 내부 zoom은 사용하지 않지만 호환성을 위해 기록\n",
    "        _append_csv(\n",
    "            zoom_csv,\n",
    "            [str(f_name), int(iter_idx),\n",
    "             f\"{float(np.linalg.norm(y_vec)):.6e}\",  # y-norm proxy\n",
    "             f\"{0.0:.6e}\", f\"{0.0:.6e}\", f\"{1.0:.6e}\",  # s_logL*, z_logL, L\n",
    "             f\"{float(a_t):.6e}\", f\"{float(a_bar):.6e}\", str(mode),\n",
    "             f\"{float(eff_scale):.6e}\"],\n",
    "            header=zoom_header\n",
    "        )\n",
    "\n",
    "    # ── Normalizer on external bounds (a_phi, r, rho) ───────────\n",
    "    N = boundary(bounds_ext)            # expects [(L,U), ...]\n",
    "    # (권장 개선) 시작점 feasible 투영: k*r가 상한을 넘으면 r을 먼저 줄인 뒤 a_phi 보정\n",
    "    U_a = bounds_ext[0][1]\n",
    "    if k * x0[1] > U_a:\n",
    "        x0[1] = min(x0[1], U_a / k)\n",
    "    # a_phi ≥ k*r 보장\n",
    "    if x0[0] < k * x0[1]:\n",
    "        x0 = np.array([np.nextafter(k * x0[1], np.inf), x0[1], x0[2]], float)\n",
    "    x0 = _proj_bounds_ext(x0)\n",
    "    y0 = N.encode(x0)                   # y∈[0,1]^3\n",
    "\n",
    "    # ── penalty (hinge^2), weight adapted at first eval ─────────\n",
    "    penalty_lambda = None\n",
    "    def _penalty_from_scale(f_abs):\n",
    "        # f 스케일에 맞춘 보수적 가중치 (경험적)\n",
    "        return max(1.0, 10.0 * float(f_abs))\n",
    "\n",
    "    # ── objective in y-space (called by optimizer) ──────────────\n",
    "    nfev = 0\n",
    "    eff_scale = scale_base * 1.0\n",
    "    def f_y(y):\n",
    "        \"\"\"Objective over normalized y. Handles boundary, penalty, logging.\"\"\"\n",
    "        nonlocal nfev, penalty_lambda\n",
    "        y = np.asarray(y, float)\n",
    "        x_ext = N.decode(y)\n",
    "        x_ext = _proj_bounds_ext(x_ext)\n",
    "\n",
    "        f_phys, res = _ensure_cached(x_ext)\n",
    "        if penalty_lambda is None:\n",
    "            penalty_lambda = _penalty_from_scale(abs(f_phys))\n",
    "\n",
    "        g = max(0.0, (k * x_ext[1]) - x_ext[0])  # violation if k*r > a_phi\n",
    "        f_used = f_phys + penalty_lambda * (g ** 2)\n",
    "\n",
    "        # CSV logging (per evaluation)\n",
    "        _log_params_and_analysis(x_ext, f_used, res, eff_scale)\n",
    "        _last_eval.update({'x_ext': x_ext.copy(), 'f': f_phys, 'res': res})\n",
    "        nfev += 1\n",
    "        if disp:\n",
    "            print(f\"{method}: a_phi={x_ext[0]:.6e}, r={x_ext[1]:.6e}, rho={x_ext[2]:.4f}, \"\n",
    "                  f\"N0={N0}, dN={delta_N}, ring={ring_num}, obj={f_used:.6e}\", flush=True)\n",
    "        return f_used\n",
    "\n",
    "    # ── gradient source for Bridge (1P bootstrap → 2P) ─────────\n",
    "    grad_1p = Forward1P(eps_rel=1e-3, K=2)     # first call only\n",
    "    grad_2p = Central2P(eps_rel=1e-4, K=6)     # subsequent calls\n",
    "\n",
    "    # ── configs (presets mapped from 'method') ──────────────────\n",
    "    method_l = str(method).lower().strip()\n",
    "    if method_l in (\"trust-constr\", \"trust\", \"tc\"):\n",
    "        step_cfg = StepperConfig(improve_only=True, initial_delta=0.25, backtrack_max=10)\n",
    "    elif method_l in (\"l-bfgs-b\", \"lbfgsb\", \"lbfgs\"):\n",
    "        step_cfg = StepperConfig(improve_only=True, initial_delta=0.15, backtrack_max=12)\n",
    "    else:  # \"slsqp\" default-like\n",
    "        step_cfg = StepperConfig(improve_only=True, initial_delta=0.20, backtrack_max=12)\n",
    "\n",
    "    bridge_cfg = BridgeConfig(\n",
    "        seed=None, bootstrap_K1p=2, central_K2p=6,\n",
    "        eps_rel_1p=1e-3, eps_rel_2p=1e-4\n",
    "    )\n",
    "    precond_cfg = PreconditionerConfig(enable=True)\n",
    "\n",
    "    # ── optimizer construction ──────────────────────────────────\n",
    "    optimizer = OnePointOptimizer(\n",
    "        f_fn=f_y, normalizer=N,\n",
    "        grad_source_boot=grad_1p, grad_source_main=grad_2p,\n",
    "        bridge_cfg=bridge_cfg, stepper_cfg=step_cfg, precond_cfg=precond_cfg\n",
    "    )\n",
    "\n",
    "    # ── run loop (improve-only acceptance; ftol & stall criteria) ─\n",
    "    maxiter = 300 if method_l == \"slsqp\" else (200 if method_l.startswith(\"trust\") else 500)\n",
    "    ftol = 1e-5\n",
    "    stall_max = 25\n",
    "\n",
    "    y = y0.copy()\n",
    "    f_best = f_y(y)            # triggers first eval & sets penalty_lambda\n",
    "    y_best = y.copy()\n",
    "    nit = 0\n",
    "    stall = 0\n",
    "\n",
    "    _log_zoom(nit, y, mode=\"NEUTRAL\", eff_scale=eff_scale)\n",
    "\n",
    "    while nit < maxiter:\n",
    "        nit += 1\n",
    "        y_new, f_new, info = optimizer.step(y)   # (y_next, f_next, info dict)\n",
    "\n",
    "        # Improve-only acceptance (external guard)\n",
    "        if f_new < f_best - ftol * max(1.0, abs(f_best)):\n",
    "            y, f_best = y_new, f_new\n",
    "            y_best = y_new.copy()\n",
    "            stall = 0\n",
    "            mode = \"IN\"\n",
    "        else:\n",
    "            stall += 1\n",
    "            mode = \"HOLD\"\n",
    "\n",
    "        # zoom-history compatibility log (L=1.0 fixed)\n",
    "        a_t  = float(info.get(\"a_t\", 0.0)) if isinstance(info, dict) else 0.0\n",
    "        a_bar= float(info.get(\"a_bar\", 0.0)) if isinstance(info, dict) else 0.0\n",
    "        _log_zoom(nit, y, mode=mode, eff_scale=eff_scale, a_t=a_t, a_bar=a_bar)\n",
    "\n",
    "        # stopping rules\n",
    "        if stall >= stall_max:\n",
    "            break\n",
    "        df = info.get(\"df\", None) if isinstance(info, dict) else None\n",
    "        if (df is not None) and (abs(df) <= ftol * max(1.0, abs(f_best))):\n",
    "            break\n",
    "\n",
    "    # ── decode & finalize result ────────────────────────────────\n",
    "    x_best = N.decode(y_best)\n",
    "    # 최종 안전 투영(경계/미세 위반 방지)\n",
    "    x_best = _proj_bounds_ext(x_best)\n",
    "    if not _feasible_ineq(x_best):\n",
    "        # 종료 시 미세 투영 (a_phi ← max(a_phi, k*r))\n",
    "        x_best = np.array([max(x_best[0], k * x_best[1]), x_best[1], x_best[2]], float)\n",
    "\n",
    "    f_phys_final, _res_final = _ensure_cached(x_best)\n",
    "    g_final = max(0.0, (k * x_best[1]) - x_best[0])\n",
    "    penalty_final = (penalty_lambda if penalty_lambda is not None else 1.0) * (g_final ** 2)\n",
    "    f_used_final = f_phys_final + penalty_final\n",
    "\n",
    "    # history attach (same as legacy)\n",
    "    try:\n",
    "        hist = pd.read_csv(params_csv)\n",
    "        history_df = hist[hist['f_name'].astype(str) == str(f_name)].reset_index(drop=True)\n",
    "    except Exception:\n",
    "        history_df = None\n",
    "\n",
    "    # SciPy-like result dict\n",
    "    sol = {\n",
    "        'x': x_best,\n",
    "        'fun': float(f_used_final),\n",
    "        'nit': int(nit),\n",
    "        'nfev': int(nfev),\n",
    "        'success': bool(True if stall < stall_max else False),\n",
    "        'status': int(0 if stall < stall_max else 1),\n",
    "        'message': \"OK: custom optimizer converged\" if stall < stall_max else \"Stopped: stall limit\",\n",
    "        'history': history_df,\n",
    "        # extras for downstream debugging\n",
    "        'bridge': {'method_preset': method_l, 'ftol': ftol, 'stall_max': stall_max},\n",
    "        'A_info': None\n",
    "    }\n",
    "\n",
    "    # cleanup\n",
    "    _cache_drop_by_fname(f_name)\n",
    "    return sol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ddf8d7",
   "metadata": {},
   "source": [
    "### Sweep operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_params(\n",
    "    x0=None,                    # [a_phi, hole_radius, rho]  OR list of such seeds\n",
    "    *,\n",
    "    # design axes (scalars or lists; lists become sweep axes)\n",
    "    N0=None,\n",
    "    delta_N=None,\n",
    "    ring_num=None,\n",
    "    wav_lo=None, wav_hi=None,   # in nm\n",
    "    method='SLSQP',\n",
    "    # offsets / sequences (scalars or lists; per-run 가능)\n",
    "    e_eta_seq=None, o_eta_seq=None,   # list[float] or list[list[float]]\n",
    "    offsets=None,                      # dict/callable or list[…]\n",
    "    parity_mat: bool | list = True,\n",
    "    NA: float | list = 0.12,\n",
    "    scale: float | list | str = \"auto\",\n",
    "    # seed options (모두 길이 3이어야 함)\n",
    "    x0_list=None,               # list of [a_phi, hole_radius, rho]\n",
    "    x0_grid=None,               # dict {\"a_phi\":[...nm], \"hole_radius\":[...nm], \"rho\":[...]}\n",
    "    # sweeping control\n",
    "    grid_mode: str = \"product\", # {\"product\",\"zip\"}\n",
    "    order=None,\n",
    "    disp: bool = False,\n",
    "    # parallelization (Ray)\n",
    "    parallel: str = \"none\",     # {\"none\",\"ray\"}\n",
    "    max_workers: int = 0,\n",
    "    # selection (opt 모드에서만 사용)\n",
    "    obj_threshold: float = -50.0,\n",
    "    tag_template: str | None = None,\n",
    "    # NEW: 모드 선택\n",
    "    mode: str = \"geom\",         # {\"geom\",\"opt\"}\n",
    "    # NEW: per-iter opt logs 보존(기본 False; 병렬 경합/손상 방지)\n",
    "    preserve_opt_logs: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sweep runner (geom/opt).\n",
    "    - 충돌 방지: sweep 집계 CSV는 'sweep_runs_{method}_{mode}.csv'로 기록.\n",
    "    - Ray 모드 기본값: optimize_structure의 per-iter CSV는 워커 temp에 남고 폐기.\n",
    "      preserve_opt_logs=True로 중앙 CSV 4종(opt_history_*.csv, out_history_*.csv, ...)에 병합 가능.\n",
    "    \"\"\"\n",
    "    import os, tempfile, json, csv, shutil, glob\n",
    "    import numpy as np, pandas as pd\n",
    "    from itertools import product as _product\n",
    "\n",
    "    if ring_num is None or wav_lo is None or wav_hi is None:\n",
    "        raise ValueError(\"ring_num, wav_lo, wav_hi are required (scalars or lists).\")\n",
    "\n",
    "    mode = str(mode).lower()\n",
    "    if mode not in {\"geom\",\"opt\"}:\n",
    "        raise ValueError(\"mode must be 'geom' or 'opt'.\")\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _is_listlike(v):\n",
    "        return isinstance(v, (list, tuple, np.ndarray))\n",
    "\n",
    "    def _as_list(v):\n",
    "        if v is None: return [None]\n",
    "        return list(v) if _is_listlike(v) else [v]\n",
    "\n",
    "    def _pad_get(seq, i):\n",
    "        if not _is_listlike(seq): return seq\n",
    "        if len(seq) == 0: return None\n",
    "        return seq[i] if i < len(seq) else seq[-1]\n",
    "\n",
    "    def _normalize_seeds(x0, x0_list, x0_grid):\n",
    "        seeds = []\n",
    "        if x0_grid is not None:\n",
    "            a_seq = list(x0_grid.get(\"a_phi\", []))\n",
    "            r_seq = list(x0_grid.get(\"hole_radius\", []))\n",
    "            rho_seq = list(x0_grid.get(\"rho\", []))\n",
    "            if not (a_seq and r_seq and rho_seq):\n",
    "                raise ValueError(\"x0_grid must provide 'a_phi', 'hole_radius', and 'rho' sequences.\")\n",
    "            seeds = [np.array([a, r, rh], float) for a, r, rh in _product(a_seq, r_seq, rho_seq)]\n",
    "        if not seeds and x0_list is not None:\n",
    "            seeds = [np.array(s, float) for s in x0_list]\n",
    "        if not seeds and x0 is not None:\n",
    "            if _is_listlike(x0) and len(x0) > 0 and _is_listlike(x0[0]):\n",
    "                seeds = [np.array(s, float) for s in x0]\n",
    "            else:\n",
    "                seeds = [np.array(x0, float)]\n",
    "        if not seeds:\n",
    "            raise ValueError(\"Provide seeds via x0, x0_list, or x0_grid.\")\n",
    "        for s in seeds:\n",
    "            if s.size != 3:\n",
    "                raise ValueError(\"All seeds must be length-3: [a_phi, hole_radius, rho].\")\n",
    "        return seeds\n",
    "\n",
    "    def _fmt(v):\n",
    "        try: return f\"{float(v):.4g}\"\n",
    "        except: return str(v)\n",
    "\n",
    "    # external helpers from notebook:\n",
    "    # _resolve_offsets, _digest_seq, _append_csv, clear_cache, optimize_structure, geom\n",
    "    if 'geom' not in globals() and mode == 'geom':\n",
    "        raise RuntimeError(\"mode='geom' requires a global function geom(...).\")\n",
    "\n",
    "    # ---------- axes digest ----------\n",
    "    N0_L      = _as_list(N0)\n",
    "    dN_L      = _as_list(delta_N)\n",
    "    ring_L    = _as_list(ring_num)\n",
    "    wav_lo_L  = _as_list(wav_lo)\n",
    "    wav_hi_L  = _as_list(wav_hi)\n",
    "    method_L  = _as_list(method)\n",
    "    NA_L      = _as_list(NA)\n",
    "    scale_L   = _as_list(scale)\n",
    "    parity_L  = _as_list(parity_mat)\n",
    "    e_seq_L   = _as_list(e_eta_seq)\n",
    "    o_seq_L   = _as_list(o_eta_seq)\n",
    "    offsets_L = _as_list(offsets)\n",
    "\n",
    "    seeds = _normalize_seeds(x0, x0_list, x0_grid)\n",
    "\n",
    "    default_order = ['method','scale','N0','delta_N','ring_num','wav_lo','wav_hi',\n",
    "                     'NA','parity_mat','e_eta_seq','o_eta_seq','offsets']\n",
    "    loop_order = order or default_order\n",
    "\n",
    "    def _axis_values(name):\n",
    "        return {\n",
    "            'method':     method_L,\n",
    "            'scale':      scale_L,\n",
    "            'N0':         N0_L,\n",
    "            'delta_N':    dN_L,\n",
    "            'ring_num':   ring_L,\n",
    "            'wav_lo':     wav_lo_L,\n",
    "            'wav_hi':     wav_hi_L,\n",
    "            'NA':         NA_L,\n",
    "            'parity_mat': parity_L,\n",
    "            'e_eta_seq':  e_seq_L,\n",
    "            'o_eta_seq':  o_seq_L,\n",
    "            'offsets':    offsets_L,\n",
    "        }[name]\n",
    "\n",
    "    tasks = []\n",
    "    if grid_mode.lower() == \"product\":\n",
    "        pools = [(_axis_values(n), n) for n in loop_order]\n",
    "        for vals in _product(*[p[0] for p in pools]):\n",
    "            d = {pools[i][1]: vals[i] for i in range(len(pools))}\n",
    "            tasks.append(d)\n",
    "    else:  # \"zip\"\n",
    "        lengths = [len(_axis_values(n)) for n in default_order if _is_listlike(_axis_values(n))]\n",
    "        Lmax = max(lengths) if lengths else 1\n",
    "        for i in range(Lmax):\n",
    "            d = dict(\n",
    "                method     = _pad_get(method_L, i),\n",
    "                scale      = _pad_get(scale_L, i),\n",
    "                N0         = _pad_get(N0_L, i),\n",
    "                delta_N    = _pad_get(dN_L, i),\n",
    "                ring_num   = _pad_get(ring_L, i),\n",
    "                wav_lo     = _pad_get(wav_lo_L, i),\n",
    "                wav_hi     = _pad_get(wav_hi_L, i),\n",
    "                NA         = _pad_get(NA_L, i),\n",
    "                parity_mat = _pad_get(parity_L, i),\n",
    "                e_eta_seq  = _pad_get(e_seq_L, i),\n",
    "                o_eta_seq  = _pad_get(o_seq_L, i),\n",
    "                offsets    = _pad_get(offsets_L, i),\n",
    "            )\n",
    "            tasks.append(d)\n",
    "\n",
    "    # 태그 템플릿\n",
    "    if tag_template is None:\n",
    "        tag_template = \"sw_{i}_{method}_N{N0}_dN{dN}_R{R}_a{a}_r{r}_rho{rho}_sc{sc}_NA{NA}_par{par}_eh{eh}_oh{oh}\"\n",
    "\n",
    "    # geom 라벨 보정\n",
    "    if mode == \"geom\" and (method_L == ['SLSQP'] or str(method_L[0]).lower() == 'slsqp'):\n",
    "        method_L = ['geom']\n",
    "\n",
    "    def _make_tag(i, t, seed, eh, oh):\n",
    "        return tag_template.format(\n",
    "            i=i, method=t['method'],\n",
    "            N0=int(t['N0']), dN=int(t['delta_N']), R=int(t['ring_num']),\n",
    "            a=_fmt(seed[0]), r=_fmt(seed[1]), rho=_fmt(seed[2]), sc=_fmt(t['scale']),\n",
    "            NA=_fmt(t['NA']), par=str(bool(t['parity_mat'])),\n",
    "            eh=str(eh), oh=str(oh),\n",
    "        )\n",
    "\n",
    "    # ---- sweep-run aggregated CSV (충돌 방지: 이름 분리) ----\n",
    "    method0 = str(method_L[0])\n",
    "    runs_csv = f'sweep_runs_{method0}_{mode}.csv'\n",
    "    print(f\"SWEEP starting: {len(tasks)*len(seeds)} runs (grid_mode={grid_mode}, parallel={parallel}, unit=nm, mode={mode})\")\n",
    "\n",
    "    # Core runner\n",
    "    def _run_one(seed, t, idx, collect_logs=False, logs_sink_dir=None):\n",
    "        # offsets/eta 확정\n",
    "        e_seq_run, o_seq_run = _resolve_offsets(\n",
    "            int(t['ring_num']),\n",
    "            t.get('e_eta_seq'), t.get('o_eta_seq'), t.get('offsets')\n",
    "        )\n",
    "        eh, oh = _digest_seq(e_seq_run), _digest_seq(o_seq_run)\n",
    "        ftag = _make_tag(idx, t, seed, eh, oh)\n",
    "\n",
    "        # per-iter 로그 수집용\n",
    "        collected = {}\n",
    "\n",
    "        if mode == \"opt\":\n",
    "            sol = optimize_structure(\n",
    "                np.array(seed, float), int(t['N0']), int(t['delta_N']), int(t['ring_num']),\n",
    "                float(t['wav_lo']), float(t['wav_hi']),\n",
    "                f_name=ftag, method=str(t['method']), disp=disp,\n",
    "                e_eta_seq=e_seq_run, o_eta_seq=o_seq_run, offsets=None,\n",
    "                parity_mat=bool(t['parity_mat']), NA=float(t['NA']),\n",
    "                scale=t['scale']\n",
    "            )\n",
    "            x = np.array(sol['x'], float) if isinstance(sol, dict) else np.array(sol.x, float)\n",
    "            obj = float(sol['fun'] if isinstance(sol, dict) else sol.fun)\n",
    "            res = sol.get('res', {}) if isinstance(sol, dict) else {}\n",
    "        else:\n",
    "            nm = 1e-9\n",
    "            a_phi_nm, hole_nm, rho = map(float, seed)\n",
    "            res = geom(\n",
    "                a_phi_nm*nm, hole_nm*nm,\n",
    "                int(t['N0']), int(t['delta_N']), int(t['ring_num']),\n",
    "                float(t['wav_lo'])*nm, float(t['wav_hi'])*nm,\n",
    "                e_eta_seq=e_seq_run, o_eta_seq=o_seq_run,\n",
    "                parity_mat=bool(t['parity_mat']), rho=float(rho),\n",
    "                NA=float(t['NA']), f_name=str(ftag)\n",
    "            )\n",
    "            x = np.array([a_phi_nm, hole_nm, rho], float)\n",
    "            obj = np.nan\n",
    "\n",
    "        # (옵션) 워커 temp → 중앙 병합용 per-iter 로그 수집\n",
    "        if collect_logs and logs_sink_dir and mode == \"opt\":\n",
    "            patterns = [\n",
    "                f'opt_history_{t[\"method\"]}_params.csv',\n",
    "                f'out_history_{t[\"method\"]}_analysis.csv',\n",
    "                f'opt_history_{t[\"method\"]}_obj.csv',\n",
    "                f'zoom_history_{t[\"method\"]}.csv'\n",
    "            ]\n",
    "            for p in patterns:\n",
    "                if os.path.exists(p):\n",
    "                    with open(p, 'r', newline='') as f:\n",
    "                        collected[p] = f.read()\n",
    "\n",
    "        # 표준 메트릭 추출(있으면 기록)\n",
    "        CE_flat  = float(res['CE_flat']) if isinstance(res, dict) and 'CE_flat' in res else np.nan\n",
    "        T_norm   = float(res['T_norm'])  if isinstance(res, dict) and 'T_norm'  in res else np.nan\n",
    "        purcell  = float(res['purcell']) if isinstance(res, dict) and 'purcell' in res else np.nan\n",
    "\n",
    "        row = {\n",
    "            'f_name': ftag,\n",
    "            'a_phi': float(x[0]), 'hole_radius': float(x[1]),\n",
    "            'N0': int(t['N0']), 'delta_N': int(t['delta_N']), 'ring_num': int(t['ring_num']),\n",
    "            'wav_lo': float(t['wav_lo']), 'wav_hi': float(t['wav_hi']),\n",
    "            'NA': float(t['NA']), 'parity_mat': bool(t['parity_mat']), 'rho': float(x[2]),\n",
    "            'e_hash': eh, 'o_hash': oh,\n",
    "            'unit_length': 'nm', 'scale_m_per_unit': np.nan,\n",
    "            'obj': obj,\n",
    "            'CE_flat': CE_flat, 'T_norm': T_norm, 'purcell': purcell,\n",
    "            'res_json': json.dumps(res, default=float) if isinstance(res, dict) else \"\",\n",
    "        }\n",
    "        return row, collected\n",
    "\n",
    "    # ---- 실행 (단일 or Ray) ----\n",
    "    results = []\n",
    "    collected_logs = []  # (파일명, 내용)\n",
    "    header = ['f_name','a_phi','hole_radius','N0','delta_N','ring_num',\n",
    "              'wav_lo','wav_hi','NA','parity_mat','rho','e_hash','o_hash',\n",
    "              'unit_length','scale_m_per_unit','obj','CE_flat','T_norm','purcell','res_json']\n",
    "\n",
    "    if parallel.lower() == \"ray\":\n",
    "        try:\n",
    "            import ray\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"parallel='ray' requested but Ray is not installed.\") from e\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(ignore_reinit_error=True, include_dashboard=False,\n",
    "                     num_cpus=(None if max_workers <= 0 else max_workers))\n",
    "\n",
    "        @ray.remote\n",
    "        def _ray_worker(seed, t, idx, preserve_logs):\n",
    "            import numpy as _np, os, tempfile, json, csv\n",
    "            # temp 디렉토리에서 충돌 없이 실행\n",
    "            with tempfile.TemporaryDirectory() as _td:\n",
    "                _cwd = os.getcwd()\n",
    "                try:\n",
    "                    os.chdir(_td)\n",
    "                    row, logs = _run_one(_np.array(seed, float), t, idx,\n",
    "                                         collect_logs=bool(preserve_logs),\n",
    "                                         logs_sink_dir=_cwd)\n",
    "                    return row, logs\n",
    "                finally:\n",
    "                    os.chdir(_cwd)\n",
    "\n",
    "        futs, idx = [], 0\n",
    "        for t in tasks:\n",
    "            if t.get('method') is None:\n",
    "                t['method'] = method_L[0]\n",
    "            for s in seeds:\n",
    "                futs.append(_ray_worker.remote(np.array(s, float), dict(t), idx, preserve_opt_logs))\n",
    "                idx += 1\n",
    "        for fut in futs:\n",
    "            row, logs = ray.get(fut)\n",
    "            results.append(row)\n",
    "            if preserve_opt_logs and logs:\n",
    "                for k, v in logs.items():\n",
    "                    collected_logs.append((k, v))\n",
    "    else:\n",
    "        idx = 0\n",
    "        for t in tasks:\n",
    "            if t.get('method') is None:\n",
    "                t['method'] = method_L[0]\n",
    "            for s in seeds:\n",
    "                row, logs = _run_one(np.array(s, float), t, idx,\n",
    "                                     collect_logs=False, logs_sink_dir=None)\n",
    "                results.append(row)\n",
    "                idx += 1\n",
    "\n",
    "    # ---- sweep 집계 CSV에 기록(충돌 방지된 단일 파일) ----\n",
    "    if results:\n",
    "        need_header = (not os.path.exists(runs_csv)) or os.path.getsize(runs_csv) == 0\n",
    "        with open(runs_csv, 'a', newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            if need_header:\n",
    "                w.writerow(header)\n",
    "            for row in results:\n",
    "                w.writerow([row.get(k, \"\") for k in header])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=header)\n",
    "\n",
    "    # 후보(candidates) 선별\n",
    "    if mode == \"opt\":\n",
    "        df['obj'] = pd.to_numeric(df['obj'], errors='coerce')\n",
    "        df = df.dropna(subset=['obj'])\n",
    "        best_idx = df.groupby('f_name')['obj'].idxmin()\n",
    "        reduced = df.loc[best_idx].reset_index(drop=True)\n",
    "        candidates = reduced[reduced['obj'] <= obj_threshold].copy()\n",
    "    else:\n",
    "        candidates = df.reset_index(drop=True)\n",
    "\n",
    "    out_csv = f'sweep_candidates_{method0}_{mode}.csv'\n",
    "    candidates.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nSaved sweep runs to {runs_csv} (rows={len(df)})\", flush=True)\n",
    "    print(f\"Saved candidates to {out_csv} (rows={len(candidates)})\", flush=True)\n",
    "\n",
    "    # (옵션) per-iter 로그 중앙 병합\n",
    "    if preserve_opt_logs and collected_logs:\n",
    "        # 파일명별로 헤더 중복 제거하며 append\n",
    "        def _append_raw_lines(dst, raw_text):\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True) if os.path.dirname(dst) else None\n",
    "            write_header = (not os.path.exists(dst)) or os.path.getsize(dst) == 0\n",
    "            lines = raw_text.splitlines()\n",
    "            if not lines:\n",
    "                return\n",
    "            with open(dst, 'a', newline='') as f:\n",
    "                if write_header:\n",
    "                    f.write(lines[0] + '\\n')\n",
    "                for ln in (lines[1:] if write_header else lines[1:]):\n",
    "                    if ln.strip():\n",
    "                        f.write(ln + '\\n')\n",
    "\n",
    "        # 대상 파일 4종\n",
    "        for fname, raw in collected_logs:\n",
    "            _append_raw_lines(fname, raw)\n",
    "        print(\"Merged per-iter optimize logs from workers into central CSVs.\", flush=True)\n",
    "\n",
    "    clear_cache()\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f2331",
   "metadata": {},
   "source": [
    "- 단일 최적화\n",
    "sol = optimize_structure(\n",
    "    x0=[180.0, 60.0, 0.5],     # 길이 3 고정\n",
    "    N0=18, delta_N=14, ring_num=14,\n",
    "    wavelength_i=1270.0, wavelength_f=1320.0,\n",
    "    f_name=\"demo_len3\",\n",
    "    offsets=dict(amp_even=0.12, mode=\"arch\"),\n",
    "    disp=False\n",
    ")\n",
    "\n",
    "\n",
    "- 스윕 (Ray 병렬 예)\n",
    "winners = sweep_params(\n",
    "    x0_grid={\"a_phi\":[160,180,200], \"hole_radius\":[55,65], \"rho\":[0.3,0.5,0.7]},\n",
    "    N0=[16,18], delta_N=[10,12], ring_num=14,\n",
    "    wav_lo=1270.0, wav_hi=1320.0,\n",
    "    offsets=[dict(amp_even=0.10,mode=\"arch\"), dict(amp_even=0.15,mode=\"arch\")],\n",
    "    parallel=\"ray\", max_workers=8, disp=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573153a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = sweep_params(\n",
    "    x0_grid={\"a_phi\":[211.1827], \"hole_radius\":[75.23277],\n",
    "             \"rho\": [0.492]},\n",
    "    N0=[18], delta_N=[14], ring_num=[14],\n",
    "    wav_lo=[1270.0], wav_hi=[1320.0],\n",
    "    e_eta_seq=[0],\n",
    "    offsets=None, parity_mat=[True], NA=[0.12], scale=[\"auto\"],\n",
    "    method=\"trust-constr\",   \n",
    "    mode=\"opt\",               \n",
    "    obj_threshold=1e9,        \n",
    "    grid_mode=\"product\",\n",
    "    parallel=\"none\", disp=False\n",
    ")\n",
    "print(candidates[['f_name','obj','CE_flat','T_norm','purcell']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcb350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEEP starting: 9 runs (grid_mode=product, parallel=none, unit=nm, mode=geom)\n"
     ]
    }
   ],
   "source": [
    "candidates = sweep_params(\n",
    "    x0_grid={\"a_phi\":[211.1827], \"hole_radius\":[75.23277],\n",
    "             \"rho\": np.arange(0.490, 0.494, 0.0005)},\n",
    "    N0=[18], delta_N=[14], ring_num=[14],\n",
    "    wav_lo=[1270.0], wav_hi=[1320.0],\n",
    "    e_eta_seq=[1/3, 2/3],      \n",
    "    offsets=None, parity_mat=[True], NA=[0.12], scale=[\"auto\"],\n",
    "    method=\"geom\",             \n",
    "    mode=\"geom\",\n",
    "    grid_mode=\"product\", parallel=\"none\", disp=False\n",
    ")\n",
    "print(candidates[['f_name','CE_flat','T_norm','purcell']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
